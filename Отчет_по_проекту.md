# Отчет по проекту "Анализатор курсов валют"

## 1. Описание проекта
Данный проект представляет собой полноценное решение для автоматизированного сбора (парсинга), хранения, обработки и визуализации курсов валют по отношению к российскому рублю. Данные извлекаются с официального сайта Центрального банка РФ. 

Проект разработан с учетом требований модульности, читаемости (PEP 8) и надежности. Главная ценность — наличие готового и интуитивно понятного дашборда с интерактивными графиками, который обновляется на основе собираемых данных.

## 2. Архитектура и структура проекта
Проект разделен на несколько независимых файлов, каждый из которых выполняет свою специфическую задачу:

### `scraper.py` — Сборщик ежедневных данных
*   **За что отвечает:** Модуль подключения к сайту ЦБ РФ и парсинга текущих курсов валют.
*   **Как работает:** Делает HTTP-запрос по URL `cbr.ru/currency_base/daily/`. С помощью библитеки `BeautifulSoup4` извлекает HTML-таблицу.
*   **Особенности (фишки):** ЦБ РФ иногда предоставляет курсы валют за 10, 100 или 1000 единиц (например, японская иена). Скрипт интеллектуально делит курс на "Номинал", приводя все значения к стоимости **за 1 единицу** валюты. Полученные данные (с временной меткой) аккуратно дописываются в файл `data/currency_history.csv` без удаления предыдущей истории благодаря логике библиотеки `pandas`.

### `scheduler_run.py` — Фоновый планировщик
*   **За что отвечает:** Автоматизация процесса сбора данных без участия пользователя.
*   **Как работает:** Использует легковесную библиотеку `schedule`. В бесконечном цикле каждые 5 часов скрипт импортирует и запускает функцию из `scraper.py`. Это позволяет собирать базу данных для аналитики в реальном времени.

### `fetch_history.py` — Массовый загрузчик исторических данных
*   **За что отвечает:** Скрипт для первичной инициализации базы данных. Позволяет не ждать месяцы работы `scheduler_run.py`, а за пару минут собрать историю.
*   **Как работает:** Делает обращения к скрытому XML-шлюзу ЦБ РФ (`XML_dynamic.asp`). Перебирает 20 самых популярных валют по их уникальным ID (например, R01235 для Доллара).
*   **Особенности (фишки):** Скачивает и агрегирует данные за **последние 3 года** (около 740 записей по каждой валюте) и сохраняет их в `currency_history.csv`. Парсинг реализован через встроенную библиотеку `xml.etree.ElementTree`, что обеспечивает высокую скорость обработки древовидных XML-ответов сервера.

### `app.py` — Пользовательский графический интерфейс (Dashboard)
*   **За что отвечает:** Визуализация собранных данных, анализ и предоставление интерфейса взаимодействия пользователю.
*   **Как работает:** Разработан на фреймворке `Streamlit`. Загружает CSV-файл (используется кэширование `@st.cache_data` для быстродействия), фильтрует 20 закрепленных валют и выводит их.
*   **Особенности (фишки):**
    *   **Ежемесячная агрегация данных:** Чтобы графики за 3 года не выглядели "шумными", скрипт группирует исторические данные по каждому месяцу и берет последнее значение. Это позволяет видеть плавные тренды.
    *   **Кастомная стилизация:** Встроен кастомный блок CSS, который принудительно включает профессиональную "Тёмную тему" для всего приложения и графиков.
    *   **Экспорт данных (Кнопки скачивания):** В приложении реализована возможность экспорта только самых свежих (последних) курсов в формате **CSV** и **JSON**. 
    *   **Решение проблемы кодировок:** CSV формируется с кодировкой `cp1251` и разделителем `;`, что позволяет открывать файл в русскоязычном Excel без иероглифов. Для JSON включен `indent=4` для красивого форматирования кода в редакторах.

### `tests/test_scraper.py` — Юнит-тестирование
*   **За что отвечает:** Проверка работоспособности парсера.
*   **Как работает:** С помощью фреймворка `pytest` проверяет, что парсер возвращает словарь с правильными ключами (`Currency_Name`, `Rate_to_RUB`), а типы данных корректны (строки и float-значения).

---

## 3. Используемые библиотеки и технологии

*   **`requests`:** Для отправки GET-запросов на сервер ЦБ РФ.
*   **`beautifulsoup4`:** Для парсинга и навигации по HTML-дереву (стандартное решение для веб-скрапинга).
*   **`pandas`:** Главный инструмент для работы с данными. Незаменим для сохранения данных в CSV/JSON без потерь, а также для "умной" агрегации данных по месяцам (группировка).
*   **`streamlit`:** Фреймворк для мгновенного создания веб-интерфейсов для Data Science проектов. Позволяет обойтись без написания маршрутизаторов, HTML и JS на фронтенде.
*   **`plotly`:** Библиотека для построения красивых, интерактивных графиков, где можно наводить курсор на точки и видеть точные значения курса.
*   **`schedule`:** Библиотека для создания Cron-подобных задач на чистом Python (запуск парсинга по времени).
*   **`pytest`:** Инфраструктура для написания и запуска тестов.

## 4. Поэтапный процесс разработки
1.  **Проектирование и сбор данных:** На первом этапе был изучен сайт ЦБ РФ, написан HTML-парсер (`scraper.py`) для извлечения сырых данных и сохранения их в CSV. Были решены проблемы деления на "номинал" валюты.
2.  **Автоматизация:** Для поддержания данных в актуальном виде был внедрен скрипт `scheduler_run.py`.
3.  **Обогащение данных:** Было принято решение не собирать историю с нуля, а написать агрегатор `fetch_history.py`, который выгрузил историю курсов за последние 3 года через XML-API центробанка.
4.  **Разработка интерфейса:** С помощью Streamlit был создан веб-дашборд (`app.py`), который читает базу данных и строит динамические метрики и диаграммы (Plotly).
5.  **Доработка интерфейса (UX/UI):** Реализована темная стилизация интерфейса для современного вида, встроена функция выгрузки (CSV/JSON) с учетом правильных кодировок (чтобы избежать артефактов в Excel), внедрена логика ежемесячного сглаживания графиков.
6.  **Написание тестов:** На последнем этапе были написаны Unit-тесты для обеспечения 100% работоспособности скрипта сбора.
